{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9723d505",
   "metadata": {},
   "source": [
    "# RAG로 AI소믈리에 wine pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4145a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5a0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05c51b",
   "metadata": {},
   "source": [
    "# LLM을 통한 요리 정보 해석\n",
    "- 이미지 -> 맛과 풍미 (image to text)\n",
    "- 입력 : 요리 이미지(url)\n",
    "- 출력 : 요리명, 요리에 대한 풍미 설명\n",
    "- 함수로 정의한 다음, RunnableLambda 객체 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def desccribe_dish_flavor(input_data):\n",
    "\n",
    "    prompt = ChatPromptTemplate([\n",
    "        (\"system\", \"\"\"\n",
    "    You are a culinary expert who analyzes food images.\n",
    "    \n",
    "    When a user provides an image of a dish:\n",
    "\n",
    "    - “Identify the commonly accepted name of the dish.”\n",
    "\n",
    "    - “Describe the flavor profile clearly and concisely, focusing on the cooking method, texture, aroma, and balance of taste.\n",
    "    If there is uncertainty, base your judgment on the most likely dish, avoid definitive statements, and maintain a professional tone.”\n",
    "        \"\"\"),\n",
    "        HumanMessagePromptTemplate.from_template([\n",
    "            {\"text\": \"\"\"아래의 이미지 요리에 대한 요리명과 요리의 풍미를 설명해 주세요.\n",
    "            출력형태:\n",
    "            요리명:\n",
    "            요리의 풍미:\n",
    "            \"\"\"},\n",
    "            {\"image_url\": \"{image_url}\"} # image_url는 정해줘 있음.\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.1,\n",
    "        api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # LCEL (Langchain Expression Language)\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e123de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# 함수 전달 인자로 func <- 함수 넣기\n",
    "r1 = RunnableLambda(desccribe_dish_flavor)\n",
    "\n",
    "input_data = {\"image_url\" :\"https://www.sbfoods-worldwide.com/ko/recipes/deq4os00000008l9-img/10_Stake_A.jpg\"}\n",
    "res = r1.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bf9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요리명: 스테이크 (또는 구운 채소를 곁들인 스테이크)\n",
      "\n",
      "요리의 풍미:\n",
      "이 요리는 팬에 구워졌거나 그릴에 조리된 것으로 보이는 스테이크가 주를 이루며, 겉은 노릇하게 시어링 되어 풍부한 육향과 캐러멜화된 풍미를 선사하고, 속은 촉촉하고 부드러운 육즙이 가득할 것으로 예상됩니다. 미디엄 레어에서 미디엄 굽기로 보이는 단면은 부드러운 식감을 암시합니다.\n",
      "\n",
      "곁들여진 구운 아스파라거스와 버섯은 각각 신선하고 아삭한 식감과 촉촉하고 쫄깃한 식감, 그리고 흙내음 가득한 고유의 향을 더하며, 구운 토마토는 살짝 터지면서 상큼한 단맛과 산미를 제공하여 전체 요리의 균형을 잡아줍니다. 로즈마리 허브는 은은하게 퍼지는 향긋하고 쌉쌀한 아로마로 식욕을 돋우며, 스테이크의 깊은 맛을 더욱 풍성하게 할 것입니다.\n",
      "\n",
      "사진상의 녹색 소스(와사비나 허브 페이스트로 추정)는 스테이크의 기름진 맛을 개운하게 잡아주거나, 신선한 허브의 향을 더해 전체적으로 savory함과 fresh함이 조화를 이루는 복합적인 맛의 경험을 제공할 것으로 보입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2251b42",
   "metadata": {},
   "source": [
    "# 요리에 가장 잘어울리는 wine top 5 검색\n",
    "- pinecone 벡터 db 저장되어 있음\n",
    "- index : wine-reviews, namespace : wine-reviews-ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d4902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문:  요리명: 스테이크 (또는 구운 채소를 곁들인 스테이크)\n",
      "\n",
      "요리의 풍미:\n",
      "이 요리는 팬에 구워졌거나 그릴에 조리된 것으로 보이는 스테이크가 주를 이루며, 겉은 노릇하게 시어링 되어 풍부한 육향과 캐러멜화된 풍미를 선사하고, 속은 촉촉하고 부드러운 육즙이 가득할 것으로 예상됩니다. 미디엄 레어에서 미디엄 굽기로 보이는 단면은 부드러운 식감을 암시합니다.\n",
      "\n",
      "곁들여진 구운 아스파라거스와 버섯은 각각 신선하고 아삭한 식감과 촉촉하고 쫄깃한 식감, 그리고 흙내음 가득한 고유의 향을 더하며, 구운 토마토는 살짝 터지면서 상큼한 단맛과 산미를 제공하여 전체 요리의 균형을 잡아줍니다. 로즈마리 허브는 은은하게 퍼지는 향긋하고 쌉쌀한 아로마로 식욕을 돋우며, 스테이크의 깊은 맛을 더욱 풍성하게 할 것입니다.\n",
      "\n",
      "사진상의 녹색 소스(와사비나 허브 페이스트로 추정)는 스테이크의 기름진 맛을 개운하게 잡아주거나, 신선한 허브의 향을 더해 전체적으로 savory함과 fresh함이 조화를 이루는 복합적인 맛의 경험을 제공할 것으로 보입니다.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m질문: \u001b[39m\u001b[33m\"\u001b[39m , query)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# top-5 검색\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:676\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    659\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    663\u001b[39m     **kwargs: Any,\n\u001b[32m    664\u001b[39m ) -> List[Document]:\n\u001b[32m    665\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[32m    666\u001b[39m \n\u001b[32m    667\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    674\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:543\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    524\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    525\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    529\u001b[39m     **kwargs: Any,\n\u001b[32m    530\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    531\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[32m    532\u001b[39m \n\u001b[32m    533\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    540\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_search_by_vector_with_score(\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    544\u001b[39m         k=k,\n\u001b[32m    545\u001b[39m         \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m,\n\u001b[32m    546\u001b[39m         namespace=namespace,\n\u001b[32m    547\u001b[39m         **kwargs,\n\u001b[32m    548\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:759\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    750\u001b[39m \n\u001b[32m    751\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_sync_client_available()\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# 사용자 프롬프트 vector화, 유사도 높은 top-5 찾기\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# 요리에 대한 풍미(설명) 들어오면,\n",
    "# 벡터 db에 인덱싱할 때 사용한 동일 임베딩 모델을 사용해서 임베딩 벡터 생성\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model = OPENAI_EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "# 벡터 db에서 유사도계산, top-5 검색\n",
    "vector_db = PineconeVectorStore(\n",
    "    embedding = embedding ,\n",
    "    index_name = PINECONE_INDEX_NAME ,\n",
    "    namespace = PINECONE_NAMESPACE\n",
    ")\n",
    "# 벡터 db에서 질문과 가장 유사한, top-5 검색하기\n",
    "query = res  # 질문\n",
    "print(\"질문: \" , query)\n",
    "print(\"-\"*50)\n",
    "vector_db.similarity_search(query, k=5) # top-5 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eae7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
